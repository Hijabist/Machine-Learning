# -*- coding: utf-8 -*-
"""Hijabist-FaceShape-Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13B0wwIdAub8BD1X_QP95RYasRP3FU5pB
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install opendatasets -q

!pip install dlib

# Library untuk manipulasi data dan analisis numerik
import pandas as pd         # Untuk bekerja dengan data tabular (DataFrame)
import numpy as np          # Untuk operasi numerik dan manipulasi array

# Library untuk plotting dan visualisasi data
import matplotlib.pyplot as plt  # Untuk pembuatan grafik dan plot dasar
import seaborn as sns        # Untuk visualisasi statistik yang lebih menarik

# Library untuk pengolahan dan manipulasi gambar
import cv2                   # Untuk pemrosesan gambar tingkat lanjut
from PIL import Image        # Untuk manipulasi gambar sederhana

# Library untuk operasi sistem, file, dan direktori
import os                    # Untuk operasi sistem dan manajemen file/direktori
import shutil                # Untuk operasi manipulasi file dan direktori
import zipfile               # Untuk handling file zip
import random                # Untuk generasi bilangan acak
from uuid import uuid4       # Untuk membuat identifier unik

# Library untuk augmentasi dan preprocessing gambar pada deep learning dengan Keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Library untuk progress bar saat iterasi panjang
from tqdm import tqdm

# Library untuk data loading
import opendatasets as od    # Untuk mengunduh dataset dari sumber online

# Library untuk deteksi wajah dan pengunduhan file terkait
import dlib                  # Untuk deteksi wajah
import urllib.request        # Untuk mengunduh file dari URL
import bz2                   # Untuk dekompresi file bz2

# Library untuk mengabaikan peringatan agar output lebih bersih
import warnings
warnings.filterwarnings('ignore')

# Library untuk upload file (khusus Google Colab)
from google.colab import files

"""# **Loading Dataset**"""

kaggle = {"username":"samsulpoetry","key":"7b6651928aa74134cc42ea304f4eb389"}

od.download('https://www.kaggle.com/datasets/niten19/face-shape-dataset')

# Tentukan path ke dataset yang diunduh
dataset_path = '/content/face-shape-dataset/FaceShape Dataset/'
train_path = os.path.join(dataset_path, 'training_set')
test_path = os.path.join(dataset_path, 'testing_set')
output_path = 'face-shape-hijabist'

# Buat folder output jika belum ada
if not os.path.exists(output_path):
    os.makedirs(output_path)

# Fungsi untuk menyalin file dari sumber ke tujuan
def copy_files(src_dir, dest_dir):
    for root, _, files in os.walk(src_dir):
        for file in files:
            if file.lower() == 'desktop.ini': # Tambahkan pengecekan ini
                continue # Lewati file desktop.ini
            src_file = os.path.join(root, file)
            # Buat struktur direktori yang sama di folder output
            relative_path = os.path.relpath(src_file, src_dir)
            dest_file = os.path.join(dest_dir, relative_path)
            # Pastikan direktori tujuan ada
            os.makedirs(os.path.dirname(dest_file), exist_ok=True)
            # Salin file
            shutil.copy2(src_file, dest_file)

# Salin file dari folder training
print(f"Menyalin file dari: {train_path}")
copy_files(train_path, output_path)
print("Selesai menyalin file training.")

# Salin file dari folder testing
print(f"Menyalin file dari: {test_path}")
copy_files(test_path, output_path)
print("Selesai menyalin file testing.")

print(f"Dataset gabungan disimpan di: {output_path}")

"""# **Exploratory Data Analysis (EDA)**"""

# Cek isi folder
datasets = './face-shape-hijabist'
print("\nIsi dalam dataset:")
print(os.listdir(datasets))

def count_images_per_class(dataset_path):
    """
    Mengembalikan dictionary jumlah gambar (.jpg/.png/.jpeg) per kelas dari dataset.
    """
    return {
        class_name: len([
            f for f in os.listdir(os.path.join(dataset_path, class_name))
            if os.path.isfile(os.path.join(dataset_path, class_name, f)) and
               f.lower().endswith(('.png', '.jpg', '.jpeg'))
        ])
        for class_name in os.listdir(dataset_path)
        if os.path.isdir(os.path.join(dataset_path, class_name))
    }

def plot_class_distribution(class_counts, title='Distribusi Jumlah Gambar per Kelas'):
    """
    Membuat barplot visualisasi distribusi jumlah gambar per kelas.
    """
    plt.figure(figsize=(8, 5))
    sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()), palette='viridis')
    plt.title(title)
    plt.xlabel('Kelas')
    plt.ylabel('Jumlah Gambar')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()

class_counts = count_images_per_class(datasets)
print("Distribusi jumlah gambar per kelas:", class_counts)

# Visualisasi distribusi
plot_class_distribution(class_counts, title='Distribusi Jumlah Gambar per Kelas Face Shape')

# Visualisasi contoh gambar dari setiap kelas
def show_samples(datasets, class_name, n=5):
    """
    Menampilkan n gambar pertama dari kelas tertentu.
    """
    folder = os.path.join(datasets, class_name)
    images = os.listdir(folder)[:n]  # Ambil n file gambar pertama dalam folder
    plt.figure(figsize=(15, 3))

    for i, img_file in enumerate(images):
        img_path = os.path.join(folder, img_file)
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        plt.subplot(1, n, i+1)
        plt.imshow(img)
        plt.title(class_name)
        plt.axis('off')

    plt.show()

# Loop untuk menampilkan contoh gambar dari setiap kelas yang ada di dalam folder dataset
for skin_class in os.listdir(datasets):
    show_samples(datasets, skin_class)

def check_image_sizes(dataset_path):
    """
    Mengumpulkan ukuran (width, height) semua gambar dalam dataset
    dan menampilkan statistik deskriptif ukuran gambar.
    """
    sizes = []  # List untuk menyimpan ukuran semua gambar

    for skin_class in os.listdir(dataset_path):
        class_path = os.path.join(dataset_path, skin_class)
        if not os.path.isdir(class_path):
            continue

        for file in os.listdir(class_path):
            img_path = os.path.join(class_path, file)

            try:
                # Buka gambar dan ambil ukuran (width, height)
                with Image.open(img_path) as img:
                    sizes.append(img.size)
            except Exception as e:
                # Tangani error jika file rusak atau bukan gambar
                print(f"Error membuka gambar {img_path}: {e}")

    sizes_df = pd.DataFrame(sizes, columns=['Width', 'Height'])
    print(sizes_df.describe())

    return sizes_df

# Jalankan fungsi untuk mengecek ukuran semua gambar dalam dataset
sizes_df = check_image_sizes(datasets)

"""Insight:
1. Ukuran gambar perlu di resize dan disamakan ke 224*224 agar cocok ke pretrained model
2. Gambar masih dipengaruhi bagian wajah lain, background dan komponen yang tidak sesuai untuk klasifikasi face shape maka perlu di cropping agar fokus di bagian dagu dan garis rahang saja.
3. distribusi data masih sudah seimbang. Namun, augmentasi tetap diperlukan untuk setiap kelas untuk robustness model

# **Data Preparation**

## **Cropping Image**
"""

url = 'http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2'
bz2_path = 'shape_predictor_68_face_landmarks.dat.bz2'
dat_path = 'shape_predictor_68_face_landmarks.dat'

# Download jika belum ada file 'shape_predictor_68_face_landmarks.dat'
if not os.path.exists(dat_path):
    print("Downloading predictor...")
    urllib.request.urlretrieve(url, bz2_path)

    print("Extracting predictor...")
    # bz2.BZ2File membuka file bz2 (file terkompresi)
    with bz2.BZ2File(bz2_path) as fr, open(dat_path, 'wb') as fw:
        fw.write(fr.read())

    # os.remove menghapus file bz2 yang sudah diekstrak
    os.remove(bz2_path)
    print("Done.")
else:
    print("Predictor file already exists.")

def crop_facial_features(source_dir, output_dir, predictor_path):
    """
    Crop facial features using HOG face detector with CLAHE preprocessing.
    Gambar yang gagal dalam deteksi wajah akan dihapus dan dicetak ke konsol.
    """
    detector = dlib.get_frontal_face_detector()
    predictor = dlib.shape_predictor(predictor_path)

    total_processed = 0
    total_cropped_successfully = 0
    deleted_images = []  # Menyimpan daftar gambar yang dihapus

    os.makedirs(output_dir, exist_ok=True)

    for class_name in os.listdir(source_dir):
        class_input_path = os.path.join(source_dir, class_name)
        class_output_path = os.path.join(output_dir, class_name)

        if not os.path.isdir(class_input_path):
            continue

        os.makedirs(class_output_path, exist_ok=True)

        image_files = [f for f in os.listdir(class_input_path)
                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))]

        print(f"\nProcessing class: {class_name}")
        for img_name in tqdm(image_files, desc=f"Cropping {class_name}"):
            input_path = os.path.join(class_input_path, img_name)
            output_img_path = os.path.join(class_output_path, img_name)

            total_processed += 1

            try:
                img = cv2.imread(input_path)
                if img is None:
                    # Gambar tidak bisa dibaca (mungkin rusak)
                    deleted_images.append(f"{class_name}/{img_name} (Image not readable)")
                    os.remove(input_path)
                    continue

                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

                # CLAHE: meningkatkan kontras lokal pada gambar grayscale
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                enhanced_gray = clahe.apply(gray)

                # deteksi wajah dengan HOG; parameter 1 berarti upsampling satu kali
                faces = detector(enhanced_gray, 1)

                if not faces:
                    # Tidak ada wajah terdeteksi
                    deleted_images.append(f"{class_name}/{img_name} (No face detected)")
                    os.remove(input_path)
                    continue

                face = faces[0]
                landmarks = predictor(enhanced_gray, face)
                points = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(68)])

                # Landmark untuk bagian wajah utama
                jaw = points[0:17]
                chin = points[8]
                nose_bridge_top = points[27]
                left_eyebrow_outer = points[17]
                right_eyebrow_outer = points[26]

                # Estimasi tinggi dahi berdasarkan jarak hidung ke dagu
                face_height_to_nose = chin[1] - nose_bridge_top[1]
                if face_height_to_nose <= 0:
                    face_height_to_nose = 1  # Hindari pembagian nol atau nilai negatif
                forehead_height_estimate = int(abs(face_height_to_nose) * 0.7)  # Rasio 60% ke atas dari hidung
                forehead_top_y = max(0, nose_bridge_top[1] - forehead_height_estimate)

                padding = 25 # Tambahan piksel di sekitar wajah agar crop tidak terlalu ketat
                x1 = min(jaw[:, 0].min(), left_eyebrow_outer[0]) - padding
                x2 = max(jaw[:, 0].max(), right_eyebrow_outer[0]) + padding
                y1 = forehead_top_y - padding
                y2 = chin[1] + padding

                # Crop menjadi persegi dan simetris
                width = x2 - x1
                height = y2 - y1
                side = max(width, height)  # Ambil sisi terpanjang untuk persegi
                center_x = (x1 + x2) // 2
                center_y = (y1 + y2) // 2

                x1 = max(0, center_x - side // 2)
                x2 = min(img.shape[1], center_x + side // 2)
                y1 = max(0, center_y - side // 2)
                y2 = min(img.shape[0], center_y + side // 2)

                if x1 >= x2 or y1 >= y2:
                    # Koordinat crop invalid
                    deleted_images.append(f"{class_name}/{img_name} (Invalid crop dimensions)")
                    os.remove(input_path)
                    continue

                cropped_bgr = img[y1:y2, x1:x2]
                if cropped_bgr.size == 0:
                    # Hasil crop kosong
                    deleted_images.append(f"{class_name}/{img_name} (Empty cropped image)")
                    os.remove(input_path)
                    continue

                # Simpan hasil crop sebagai RGB
                Image.fromarray(cv2.cvtColor(cropped_bgr, cv2.COLOR_BGR2RGB)).save(output_img_path)
                total_cropped_successfully += 1

            except Exception as e:
                # Jika terjadi error apapun, hapus gambar dan simpan pesan error
                deleted_images.append(f"{class_name}/{img_name} (Exception: {str(e)})")
                os.remove(input_path)

    print(f"\n--- Cropping Summary (HOG + CLAHE, Square Box) ---")
    print(f"Total images processed: {total_processed}")
    print(f"Successfully cropped: {total_cropped_successfully}")
    print(f"Deleted images due to failure: {len(deleted_images)}")

    if deleted_images:
        print("\n--- List of Deleted Images ---")
        for img in deleted_images:
            print(img)

crop_facial_features(
    source_dir='/content/face-shape-hijabist',
    output_dir='./face-cropped-hijabist',
    predictor_path='shape_predictor_68_face_landmarks.dat'
)

datasets_new = '/content/face-cropped-hijabist'

for skin_class in os.listdir(datasets_new):
    show_samples(datasets_new, skin_class)

class_counts = count_images_per_class(datasets_new)

# Cetak hasil
print("Distribusi jumlah gambar per kelas:", class_counts)

"""## **Resize Image**"""

def resize_with_padding(img, target_size=(224, 224), bg_color=(0, 0, 0)):
    """
    Resize gambar dengan mempertahankan rasio aspek dan menambahkan padding.
    """
    original_size = img.size  # (width, height)
    ratio = min(target_size[0] / original_size[0], target_size[1] / original_size[1])
    new_size = (int(original_size[0] * ratio), int(original_size[1] * ratio))
    img = img.resize(new_size, Image.Resampling.LANCZOS)

    new_img = Image.new("RGB", target_size, bg_color)
    paste_position = ((target_size[0] - new_size[0]) // 2,
                      (target_size[1] - new_size[1]) // 2)
    new_img.paste(img, paste_position)
    return new_img

def resize_images(source_dir, output_dir, target_size=(224, 224)):
    """
    Resize semua gambar dari source_dir ke ukuran target dengan padding dan simpan ke output_dir
    dengan struktur folder kelas yang sama.
    """
    print(f"\nResizing images from {source_dir} to {target_size} with padding and saving to {output_dir}...")
    total_resized = 0
    errored_files = []

    os.makedirs(output_dir, exist_ok=True)

    for class_name in os.listdir(source_dir):
        class_source_path = os.path.join(source_dir, class_name)
        class_output_path = os.path.join(output_dir, class_name)

        if not os.path.isdir(class_source_path):
            continue

        os.makedirs(class_output_path, exist_ok=True)

        print(f"  Processing class: {class_name}")
        image_files = [
            f for f in os.listdir(class_source_path)
            if f.lower().endswith(('.png', '.jpg', '.jpeg'))
        ]

        for img_file in tqdm(image_files, desc=f"    Resizing {class_name}"):
            source_img_path = os.path.join(class_source_path, img_file)

            try:
                with Image.open(source_img_path) as img:
                    if img.mode != 'RGB':
                        img = img.convert('RGB')

                    # Resize dengan padding
                    img_resized = resize_with_padding(img, target_size)

                    # Simpan ke folder output baru
                    output_img_path_jpg = os.path.splitext(os.path.join(class_output_path, img_file))[0] + ".jpg"
                    img_resized.save(output_img_path_jpg, format='JPEG', quality=95)

                    total_resized += 1

            except Exception as e:
                print(f"    Error resizing {source_img_path}: {e}")
                errored_files.append(source_img_path)

    print("\nImage resizing complete.")
    print(f"Total images resized: {total_resized}")
    if errored_files:
        print(f"Files with errors during resizing ({len(errored_files)}):")
        for f in errored_files:
            print(f"  - {f}")

resized_dataset = "./dataset-face-shape_resized"

# Jalankan fungsi resize
resize_images(datasets_new, resized_dataset, target_size=(224, 224))

for skin_class in os.listdir(resized_dataset):
    show_samples(resized_dataset, skin_class)

# Cek Ukuran Gambar
sizes_df = check_image_sizes(resized_dataset)

"""# **Data Splitting**"""

def split_dataset(source_dir, dest_dir, train_ratio=0.8):
    """
    Memisahkan dataset ke dalam folder 'train' dan 'val' dengan rasio tertentu.
    Membagi data secara stratified manual: membagi data dari setiap kelas satu per satu.
    """
    print(
        f"\nMembagi dataset dari {source_dir} ke dalam train/val "
        f"dengan rasio {int(train_ratio * 100)}/{int((1 - train_ratio) * 100)}..."
    )

    # Ambil semua nama folder kelas (diasumsikan satu kelas = satu folder)
    classes = [
        d for d in os.listdir(source_dir)
        if os.path.isdir(os.path.join(source_dir, d))
    ]

    # Buat struktur direktori tujuan: dest_dir/train/<kelas> dan dest_dir/val/<kelas>
    for split in ('train', 'val'):
        for cls in classes:
            os.makedirs(os.path.join(dest_dir, split, cls), exist_ok=True)

    # Lakukan proses per kelas (stratifikasi manual)
    for cls in classes:
        class_path = os.path.join(source_dir, cls)
        all_images = [
            f for f in os.listdir(class_path)
            if f.lower().endswith(('.png', '.jpg', '.jpeg'))
        ]

        random.shuffle(all_images)  # Mengacak urutan agar pembagian tidak bias

        # Hitung batas split sesuai rasio yang ditentukan
        split_index = int(len(all_images) * train_ratio)

        train_images = all_images[:split_index]
        val_images = all_images[split_index:]

        # Salin file ke folder masing-masing
        for split_name, images in [('train', train_images), ('val', val_images)]:
            print(f"  Processing class: {cls} ({split_name})")
            for img_name in tqdm(images, desc=f"    Copying {split_name} {cls}"):
                src = os.path.join(class_path, img_name)
                dst = os.path.join(dest_dir, split_name, cls, img_name)
                shutil.copy2(src, dst)  # copy2 mempertahankan metadata file

    print("\nPembagian dataset selesai.")

def count_images_in_folder(base_dir, split):
    """
    Menghitung jumlah gambar dalam folder 'train' dan 'val'.
    Menampilkan jumlah gambar per kelas.
    """
    path = os.path.join(base_dir, split)
    if not os.path.exists(path):
        print(f"Folder '{path}' tidak ditemukan.")
        return

    print(f"\nJumlah data di folder {split}:")
    for cls in os.listdir(path):
        cls_path = os.path.join(path, cls)
        if os.path.isdir(cls_path):
            count = len([
                f for f in os.listdir(cls_path)
                if f.lower().endswith(('.png', '.jpg', '.jpeg'))
            ])
            print(f"  - {cls}: {count} gambar")

split_dataset_path = './dataset-face-shape_split'

split_dataset(resized_dataset, split_dataset_path, train_ratio=0.8)

count_images_in_folder(split_dataset_path, 'train')
count_images_in_folder(split_dataset_path, 'val')

"""## **Augmentasi Data Training**"""

def augment_images(train_dir, target_count):
    """
    Melakukan augmentasi gambar di folder train_dir agar setiap kelas memiliki
    jumlah gambar sebanyak target_count
    """

    # Konfigurasi ImageDataGenerator untuk augmentasi gambar.
    augmentor = ImageDataGenerator(
        brightness_range=(0.8, 1.2),    # Variasi pencahayaan
        rotation_range=10,              # Rotasi hingga 15 derajat
        zoom_range=0.1,                 # Perbesar/perkecil hingga 10%
        fill_mode='nearest',            # Hindari padding aneh
    )

    # Loop untuk setiap folder kelas di direktori pelatihan
    for class_name in os.listdir(train_dir):
        class_path = os.path.join(train_dir, class_name)
        if not os.path.isdir(class_path):
            continue  # Lewati jika bukan direktori (misalnya file README)

        # Ambil daftar gambar asli (tanpa 'aug' di nama file) di kelas ini
        images = [
            f for f in os.listdir(class_path)
            if f.lower().endswith(('.jpg', '.jpeg', '.png')) and 'aug' not in f.lower()
        ]
        current_count = len(images)  # Jumlah gambar asli saat ini

        # Jika jumlah gambar sudah mencukupi, lewati augmentasi untuk kelas ini
        if current_count >= target_count:
            print(f"Kelas '{class_name}' sudah cukup: {current_count}/{target_count}")
            continue

        needed = target_count - current_count  # Jumlah gambar yang perlu ditambahkan
        # Hitung jumlah augmentasi yang dibutuhkan per gambar
        per_image_aug_count = needed // len(images) if len(images) > 0 else 0
        # Tambahkan 1 augmentasi ekstra untuk sisa jika ada
        if len(images) > 0 and needed % len(images) != 0:
             per_image_aug_count += 1

        augmented_count = 0  # Counter jumlah augmentasi yang telah dibuat

        print(f"\nAugmentasi kelas {class_name} ({needed} gambar dibutuhkan):")

        with tqdm(total=needed, desc=f"Augmenting {class_name}", unit="images") as pbar:
            # Loop setiap gambar asli dan lakukan augmentasi
            for img_name in images:
                if augmented_count >= needed:
                    break  # Stop jika augmentasi sudah cukup

                img_path = os.path.join(class_path, img_name)
                try:
                    # Buka gambar dan ubah ke array
                    img = Image.open(img_path).convert('RGB')
                    img_array = np.expand_dims(np.array(img), axis=0)

                    i = 0  # Counter untuk jumlah augmentasi dari gambar ini

                    # Lakukan augmentasi menggunakan flow generator
                    for batch in augmentor.flow(
                        img_array,
                        batch_size=1,
                        save_to_dir=None,      # Kita simpan secara manual
                        save_format='jpg'
                    ):
                        # Buat nama file unik menggunakan UUID pendek
                        unique_id = uuid4().hex[:8]
                        new_filename = f"aug_{class_name}_{unique_id}.jpg"
                        save_path = os.path.join(class_path, new_filename)

                        # Simpan hasil augmentasi
                        Image.fromarray(batch[0].astype(np.uint8)).save(save_path, format='JPEG', quality=95)

                        i += 1
                        augmented_count += 1
                        pbar.update(1) # Update progress bar

                        # Hentikan augmentasi dari gambar ini jika sudah cukup
                        if i >= per_image_aug_count or augmented_count >= needed:
                            break

                except Exception as e:
                    # Tangani error pada gambar yang rusak atau tidak bisa dibuka
                    print(f"\nError pada {img_path}: {str(e)[:50]}")
                    pbar.set_description(f"Augmenting {class_name} (Error on {os.path.basename(img_path)})")


    # Cek akhir jumlah gambar per kelas setelah augmentasi
    final_count = {
        class_name: len([
            f for f in os.listdir(os.path.join(train_dir, class_name))
            if f.lower().endswith(('.jpg', '.jpeg', '.png'))
        ])
        for class_name in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, class_name))
    }

    print("\nHasil akhir distribusi gambar:")
    for k, v in final_count.items():
        print(f"{k}: {v} gambar")

train_dir = '/content/dataset-face-shape_split/train'
augment_images(train_dir, target_count=1000)

class_counts = count_images_per_class(train_dir)
print("Distribusi jumlah gambar per kelas:", class_counts)

# Visualisasi distribusi
plot_class_distribution(class_counts, title='Distribusi Jumlah Gambar Training Face Shape')

def show_augmented_samples_per_class(train_dir, n=5):
    """
    Menampilkan n gambar augmented pertama untuk setiap kelas dalam direktori train_dir.
    """
    classes = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]

    if not classes:
        print(f"Tidak ada subdirektori (kelas) ditemukan di {train_dir}")
        return

    print("\nMenampilkan contoh gambar augmented per kelas:")

    for class_name in classes:
        folder = os.path.join(train_dir, class_name)
        # Filter gambar yang namanya diawali dengan 'aug_'
        augmented_images = [
            f for f in os.listdir(folder)
            if f.lower().startswith('aug_') and f.lower().endswith(('.png', '.jpg', '.jpeg'))
        ]

        if not augmented_images:
            print(f"\nTidak ada gambar augmented ditemukan di kelas '{class_name}'.")
            continue

        # Pilih n gambar pertama atau semua gambar jika jumlahnya kurang dari n
        images_to_show = augmented_images[:n]

        plt.figure(figsize=(15, 3))

        print(f"\nKelas '{class_name}': Menampilkan {len(images_to_show)} gambar augmented.")

        for i, img_file in enumerate(images_to_show):
            img_path = os.path.join(folder, img_file)
            try:
                img = cv2.imread(img_path)
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

                plt.subplot(1, len(images_to_show), i+1)
                plt.imshow(img)
                plt.title(f"Augmented\n{class_name}")
                plt.axis('off')
            except Exception as e:
                print(f"Gagal menampilkan gambar {img_file}: {e}")
        plt.show() # Tampilkan plot untuk kelas saat ini sebelum ke kelas berikutnya

# Ganti 'train_dir' jika path ke folder training Anda berbeda
train_dir = '/content/dataset-face-shape_split/train'

# Panggil fungsi untuk menampilkan sampel augmented per kelas
show_augmented_samples_per_class(train_dir, n=5)

!zip -r new-aug-fs-hijabist1000.zip ./dataset-face-shape_split

# Definisikan path ke file zip
zip_files_to_copy = ['new-aug-fs-hijabist1000.zip', 'last-cleaned-fs-hijabist.zip']

# Inisialisasi folder Google Drive
destination_folder = '/content/drive/MyDrive/Hijabist-datasets/'
os.makedirs(destination_folder, exist_ok=True)

# Salin setiap file zip ke Google Drive
for zip_file_path in zip_files_to_copy:
  destination_path = os.path.join(destination_folder, os.path.basename(zip_file_path))
  try:
    shutil.copy(zip_file_path, destination_path)
    print(f"'{zip_file_path}' disalin ke '{destination_path}'")
  except FileNotFoundError:
    print(f"Error: '{zip_file_path}' tidak ditemukan.")
  except Exception as e:
    print(f"Terjadi error saat menyalin '{zip_file_path}': {e}")